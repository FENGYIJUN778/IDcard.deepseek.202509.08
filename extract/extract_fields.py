import re
import torch
from PIL import Image, ImageDraw, ImageFont
from deepseek_vl2.models import DeepseekVLV2ForCausalLM, DeepseekVLV2Processor

class CardExtractor:
    def __init__(self, model_path="deepseek-ai/deepseek-vl2-tiny", device=None, dtype=None):
        """
        åˆå§‹åŒ–å¡ç‰‡æå–å™¨
        
        å‚æ•°:
            model_path: æ¨¡å‹è·¯å¾„ (é»˜è®¤ä½¿ç”¨ tiny ç‰ˆæœ¬)
            device: æŒ‡å®šè®¾å¤‡ (None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©)
            dtype: æ•°æ®ç±»å‹ (None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©)
        """
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        self.processor = DeepseekVLV2Processor.from_pretrained(model_path)
        
        # è®¾ç½®EOS token
        if not hasattr(self.processor.tokenizer, 'eos_token') or self.processor.tokenizer.eos_token is None:
            self.processor.tokenizer.eos_token = "<|endoftext|>"
        if not hasattr(self.processor.tokenizer, 'eos_token_id') or self.processor.tokenizer.eos_token_id is None:
            self.processor.tokenizer.eos_token_id = 100010
        
        # è®¾å¤‡ä¸ç²¾åº¦
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        if dtype is None:
            dtype = torch.float16 if device == "cuda" else torch.float32
            
        self.device = device
        self.dtype = dtype
        
        self.model = DeepseekVLV2ForCausalLM.from_pretrained(
            model_path,
            torch_dtype=dtype,
            device_map="auto"
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡ï¼š{self.model.device}, ç²¾åº¦ï¼š{dtype}")

    def extract_fields(self, image, prompt=None):
        """
        ä»å›¾åƒä¸­æå–å­—æ®µä¿¡æ¯
        
        å‚æ•°:
            image: PIL.Image å¯¹è±¡
            prompt: è‡ªå®šä¹‰æç¤ºè¯ (None ä½¿ç”¨é»˜è®¤æç¤º)
        
        è¿”å›:
            åŒ…å«æå–ç»“æœçš„å­—å…¸
        """
        # ä¿å­˜åŸå§‹å›¾åƒç”¨äºåç»­æ ‡æ³¨
        original_image = image.copy()
        
        # é»˜è®¤æç¤ºè¯
        if prompt is None:
            prompt = "<image>\nè¯·æå–è¿™å¼ æ—¥æœ¬åœ¨ç•™å¡ä¸Šçš„å§“åã€å›½ç±ã€åœ¨ç•™èµ„æ ¼ï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœã€‚"
        
        conversation = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": ""}
        ]
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            conversations=conversation,
            images=[image],
            return_tensors="pt",
            force_batchify=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = inputs.to(self.model.device)
        
        # è½¬æ¢å›¾åƒæ•°æ®ç±»å‹
        if self.dtype == torch.float16:
            inputs.images = inputs.images.to(torch.float16)
        elif self.dtype == torch.float32:
            inputs.images = inputs.images.to(torch.float32)
            
        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        model_inputs = {
            "input_ids": inputs.input_ids,
            "images": inputs.images,
        }
        if hasattr(inputs, 'attention_mask') and inputs.attention_mask is not None:
            model_inputs["attention_mask"] = inputs.attention_mask
        if hasattr(inputs, 'images_seq_mask') and inputs.images_seq_mask is not None:
            model_inputs["images_seq_mask"] = inputs.images_seq_mask
        if hasattr(inputs, 'images_spatial_crop') and inputs.images_spatial_crop is not None:
            model_inputs["images_spatial_crop"] = inputs.images_spatial_crop
        
        # æ¨ç†
        with torch.no_grad():
            output = self.model.generate(
                **model_inputs,
                max_new_tokens=300,
                temperature=0.1,
                do_sample=False,
                pad_token_id=self.processor.tokenizer.eos_token_id,
                eos_token_id=self.processor.tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        raw_output = self.processor.tokenizer.decode(output[0], skip_special_tokens=False)
        
        # æå–æ¨¡å‹ç”Ÿæˆçš„å†…å®¹
        start_token = "<|Assistant|>"
        start_idx = raw_output.find(start_token)
        if start_idx != -1:
            decoded_text = raw_output[start_idx + len(start_token):].strip()
        else:
            decoded_text = raw_output
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = re.sub(r'<\|[^>]+\|>', '', decoded_text).strip()
        
        # æå–ä¿¡æ¯
        extracted_info = self.extract_info(cleaned_text)
        
        # æå–è¾¹ç•Œæ¡†
        bboxes = self.extract_bboxes(raw_output)
        
        return {
            "raw_output": raw_output,
            "cleaned_text": cleaned_text,
            "extracted_info": extracted_info,
            "bboxes": bboxes,
            "original_image": original_image
        }
    
    def extract_info(self, text):
        """
        ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
        
        å‚æ•°:
            text: æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬
            
        è¿”å›:
            å­—å…¸å½¢å¼çš„ç»“æ„åŒ–ä¿¡æ¯ æˆ– åŸå§‹æ–‡æœ¬
        """
        result = {}
        # å°è¯•åŒ¹é…JSON
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                import json
                json_str = json_match.group(0)
                data = json.loads(json_str)
                return data
            except:
                pass
        
        patterns = {
            "å§“å": r'(å§“å|åå­—)[ï¼š:\s]*([^\n]+)',
            "å›½ç±": r'(å›½ç±|å›½å®¶)[ï¼š:\s]*([^\n]+)',
            "åœ¨ç•™èµ„æ ¼": r'(åœ¨ç•™èµ„æ ¼|èµ„æ ¼)[ï¼š:\s]*([^\n]+)'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, text)
            if match:
                value = match.group(2).strip()
                value = re.sub(r'\[\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+\]', '', value)
                result[key] = value
        
        return result if result else text
    
    def extract_bboxes(self, text):
        """
        ä»æ–‡æœ¬ä¸­æå–è¾¹ç•Œæ¡†åæ ‡
        
        å‚æ•°:
            text: åŒ…å«åæ ‡æ ‡è®°çš„æ–‡æœ¬
            
        è¿”å›:
            è¾¹ç•Œæ¡†åæ ‡åˆ—è¡¨ [[x_min, y_min, x_max, y_max], ...]
        """
        bboxes = []
        pattern = r'\[(\d+\.\d+),\s*(\d+\.\d+),\s*(\d+\.\d+),\s*(\d+\.\d+)\]'
        
        for match in re.finditer(pattern, text):
            try:
                coords = [float(match.group(i)) for i in range(1, 5)]
                # éªŒè¯åæ ‡å€¼åœ¨0-1ä¹‹é—´
                if all(0.0 <= coord <= 1.0 for coord in coords):
                    bboxes.append(coords)
            except:
                continue
        
        return bboxes
    
    def draw_annotations(self, image, bboxes, info=None):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†å’Œä¿¡æ¯
        
        å‚æ•°:
            image: PIL.Image å¯¹è±¡
            bboxes: è¾¹ç•Œæ¡†åæ ‡åˆ—è¡¨
            info: è¦æ˜¾ç¤ºçš„ä¿¡æ¯å­—å…¸
            
        è¿”å›:
            æ ‡æ³¨åçš„PIL.Imageå¯¹è±¡
        """
        annotated_image = image.copy()
        draw = ImageDraw.Draw(annotated_image)
        width, height = annotated_image.size
        
        # åŠ è½½å­—ä½“
        try:
            font = ImageFont.truetype("arial.ttf", 20)
        except:
            font = ImageFont.load_default()
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        for i, bbox in enumerate(bboxes):
            # è½¬æ¢åæ ‡
            pixel_bbox = self.normalize_to_pixels(bbox, width, height)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            draw.rectangle(pixel_bbox, outline="red", width=3)
            
            # æ·»åŠ æ ‡ç­¾
            label = f"åŒºåŸŸ{i+1}"
            draw.text((pixel_bbox[0], pixel_bbox[1] - 25), label, fill="red", font=font)
        
        # ç»˜åˆ¶ä¿¡æ¯
        if info and isinstance(info, dict):
            y_offset = 10
            for key, value in info.items():
                text = f"{key}: {value}"
                draw.text((10, y_offset), text, fill="blue", font=font)
                y_offset += 30
        
        return annotated_image
    
    def normalize_to_pixels(self, bbox, img_width, img_height):
        """
        å°†å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡
        
        å‚æ•°:
            bbox: å½’ä¸€åŒ–åæ ‡ [x_min, y_min, x_max, y_max]
            img_width: å›¾åƒå®½åº¦
            img_height: å›¾åƒé«˜åº¦
            
        è¿”å›:
            åƒç´ åæ ‡å…ƒç»„ (x_min, y_min, x_max, y_max)
        """
        x_min, y_min, x_max, y_max = bbox
        return (
            int(x_min * img_width),
            int(y_min * img_height),
            int(x_max * img_width),
            int(y_max * img_height)
        )