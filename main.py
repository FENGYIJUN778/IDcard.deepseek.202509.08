import os
import re
import csv
import json
import torch
from PIL import Image, ImageDraw, ImageFont
from deepseek_vl2.models import DeepseekVLV2ForCausalLM, DeepseekVLV2Processor

# ===== 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ =====
model_path = "deepseek-ai/deepseek-vl2-tiny"
print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
processor = DeepseekVLV2Processor.from_pretrained(model_path)

if not hasattr(processor.tokenizer, 'eos_token') or processor.tokenizer.eos_token is None:
    processor.tokenizer.eos_token = "<|endoftext|>"
if not hasattr(processor.tokenizer, 'eos_token_id') or processor.tokenizer.eos_token_id is None:
    processor.tokenizer.eos_token_id = 100010

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = DeepseekVLV2ForCausalLM.from_pretrained(
    model_path,
    torch_dtype=dtype,
    device_map="auto"
)
print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸï¼šãƒ‡ãƒã‚¤ã‚¹={model.device}ã€ç²¾åº¦={dtype}")

# ===== 2. ãƒ•ã‚©ãƒ«ãƒ€æº–å‚™ =====
image_folder = "images"
output_folder = "output"
os.makedirs(output_folder, exist_ok=True)

image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
if not image_files:
    print("âŒ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    exit(1)

# ===== 3. CSV åˆæœŸåŒ– =====
csv_path = "results.csv"
fieldnames = ["ãƒ•ã‚¡ã‚¤ãƒ«å", "æ°å", "ç”Ÿå¹´æœˆæ—¥", "æ€§åˆ¥", "å›½ç±", "ä½æ‰€", "åœ¨ç•™è³‡æ ¼", "åœ¨ç•™æœŸé–“", "è¨±å¯å¹´æœˆæ—¥"]
with open(csv_path, mode='w', newline='', encoding='utf-8-sig') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

# ===== 4. æŠ½å‡ºé–¢æ•° =====
def extract_info(text):
    result = {}
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group(0))
            return data
        except:
            pass
    patterns = {
        "æ°å": r'(æ°å)[ï¼š:\s]*([^\n]+)',
        "ç”Ÿå¹´æœˆæ—¥": r'(ç”Ÿå¹´æœˆæ—¥)[ï¼š:\s]*([^\n]+)',
        "æ€§åˆ¥": r'(æ€§åˆ¥)[ï¼š:\s]*([^\n]+)',
        "å›½ç±": r'(å›½ç±)[ï¼š:\s]*([^\n]+)',
        "ä½æ‰€": r'(ä½æ‰€|ä½å±…åœ°)[ï¼š:\s]*([^\n]+)',
        "åœ¨ç•™è³‡æ ¼": r'(åœ¨ç•™è³‡æ ¼)[ï¼š:\s]*([^\n]+)',
        "åœ¨ç•™æœŸé–“": r'(åœ¨ç•™æœŸé–“)[ï¼š:\s]*([^\n]+)',
        "è¨±å¯å¹´æœˆæ—¥": r'(è¨±å¯å¹´æœˆæ—¥)[ï¼š:\s]*([^\n]+)'
    }
    for key, pattern in patterns.items():
        match = re.search(pattern, text)
        if match:
            value = match.group(2).strip()
            value = re.sub(r'\[\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+\]', '', value)
            result[key] = value
    return result if result else text

def extract_bboxes(text):
    pattern = r'\[(\d+\.\d+),\s*(\d+\.\d+),\s*(\d+\.\d+),\s*(\d+\.\d+)\]'
    return [
        [float(m.group(i)) for i in range(1, 5)]
        for m in re.finditer(pattern, text)
        if all(0.0 <= float(m.group(i)) <= 1.0 for i in range(1, 5))
    ]

def normalize_to_pixels(bbox, w, h):
    return tuple(int(coord * dim) for coord, dim in zip(bbox, [w, h, w, h]))

# ===== 5. å„ç”»åƒã‚’å‡¦ç† =====
for idx, filename in enumerate(image_files):
    print(f"\nğŸ“‚ å‡¦ç†ä¸­: {filename} ({idx+1}/{len(image_files)})")
    image_path = os.path.join(image_folder, filename)
    image = Image.open(image_path).convert("RGB")
    original_image = image.copy()

    conversation = [
        {
            "role": "user",
            "content": "<image>\nã“ã®æ—¥æœ¬ã®åœ¨ç•™ã‚«ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚æ°åã€ç”Ÿå¹´æœˆæ—¥ã€æ€§åˆ¥ã€å›½ç±ã€ä½æ‰€ã€åœ¨ç•™è³‡æ ¼ã€åœ¨ç•™æœŸé–“ã€è¨±å¯å¹´æœˆæ—¥ã‚’ JSON å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        },
        {
            "role": "assistant",
            "content": ""
        }
    ]

    try:
        inputs = processor(
            conversations=conversation,
            images=[image],
            return_tensors="pt",
            force_batchify=True
        )
        inputs = inputs.to(model.device)
        inputs.images = inputs.images.to(dtype)

        model_inputs = {
            "input_ids": inputs.input_ids,
            "images": inputs.images,
        }
        if hasattr(inputs, 'attention_mask') and inputs.attention_mask is not None:
            model_inputs["attention_mask"] = inputs.attention_mask
        if hasattr(inputs, 'images_seq_mask') and inputs.images_seq_mask is not None:
            model_inputs["images_seq_mask"] = inputs.images_seq_mask
        if hasattr(inputs, 'images_spatial_crop') and inputs.images_spatial_crop is not None:
            model_inputs["images_spatial_crop"] = inputs.images_spatial_crop

        with torch.no_grad():
            output = model.generate(
                **model_inputs,
                max_new_tokens=300,
                temperature=0.1,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        raw_output = processor.tokenizer.decode(output[0], skip_special_tokens=False)
        start_token = "<|Assistant|>"
        start_idx = raw_output.find(start_token)
        decoded_text = raw_output[start_idx + len(start_token):].strip() if start_idx != -1 else raw_output
        cleaned_text = re.sub(r'<\|[^>]+\|>', '', decoded_text).strip()

        extracted_info = extract_info(cleaned_text)

        # ===== æç”» =====
        bboxes = extract_bboxes(raw_output)
        annotated_image = original_image.copy()
        draw = ImageDraw.Draw(annotated_image)
        width, height = annotated_image.size
        try:
            font = ImageFont.truetype("arial.ttf", 20)
        except:
            font = ImageFont.load_default()

        for i, bbox in enumerate(bboxes):
            pixel_box = normalize_to_pixels(bbox, width, height)
            draw.rectangle(pixel_box, outline="red", width=3)
            draw.text((pixel_box[0], pixel_box[1] - 25), f"é ˜åŸŸ{i+1}", fill="red", font=font)

        # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
        base = os.path.splitext(filename)[0]
        annotated_path = os.path.join(output_folder, f"{base}_bbox.jpg")
        annotated_image.save(annotated_path)

        if isinstance(extracted_info, dict):
            info_image = annotated_image.copy()
            draw = ImageDraw.Draw(info_image)
            y = 10
            for k, v in extracted_info.items():
                draw.text((10, y), f"{k}: {v}", fill="blue", font=font)
                y += 30
            info_path = os.path.join(output_folder, f"{base}_info.jpg")
            info_image.save(info_path)

        # ===== æ›¸ãè¾¼ã¿ =====
        with open(csv_path, mode='a', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            if isinstance(extracted_info, dict):
                row = {"ãƒ•ã‚¡ã‚¤ãƒ«å": filename}
                for k in fieldnames[1:]:
                    row[k] = extracted_info.get(k, "")
                writer.writerow(row)

        print(f"âœ… æŠ½å‡ºæˆåŠŸ: {filename}")

    except Exception as e:
        print(f"âŒ å‡¦ç†å¤±æ•—: {filename} -> {str(e)}")

print("\nğŸ‰ ã™ã¹ã¦ã®ç”»åƒå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")